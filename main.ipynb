{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import dotenv\n",
    "dotenv.load_dotenv('.env')\n",
    "\n",
    "def get_embeddings_oai(texts):\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=texts,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def recursive_knowledge(path: str, node: ast.Module): \n",
    "    sub_df = pd.DataFrame([[path, ast.unparse(node), []]], columns=['path', 'data', 'embedding']) \n",
    "    if not hasattr(node, 'body'): return\n",
    "    for item in node.body:\n",
    "        newPath = path + '>' + item.__class__.__name__\n",
    "        sub_df = pd.concat([sub_df, recursive_knowledge(newPath, item)])\n",
    "    return sub_df\n",
    "        \n",
    "def walk_file_tree(path, ext):\n",
    "    knowledge_df = pd.DataFrame(columns=['path', 'data', 'embedding'])\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            knowledge_df = pd.concat([knowledge_df, walk_file_tree(item_path, ext)])\n",
    "        elif os.path.splitext(item_path)[1] in ext:\n",
    "            with open(item_path, 'r') as f:\n",
    "                knowledge_df = pd.concat([knowledge_df, recursive_knowledge(item_path, ast.parse(f.read()))])\n",
    "    return knowledge_df\n",
    "\n",
    "def generate_knowledge_from_dir(path_in, path_out, ext=['.py']):\n",
    "    knowledge_df = walk_file_tree(path_in, ext)\n",
    "    resp = get_embeddings_oai((\"DATA_PATH: \"+knowledge_df['path']+'\\nDATA:'+knowledge_df['data']).tolist())\n",
    "    resp = [emb.embedding for emb in resp.data]\n",
    "    knowledge_df['embedding'] = resp\n",
    "    knowledge_df.to_csv(path_out, index=False)\n",
    "\n",
    "def load_knowledge(path_in):\n",
    "    knowledge_df = pd.read_csv(path_in)\n",
    "    knowledge_df['embedding'] = knowledge_df['embedding'].apply(lambda x: json.loads(x))\n",
    "    return knowledge_df\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def get_top_k_context(knowledge_df, text, k):\n",
    "    embedding = get_embeddings_oai(text).data[0].embedding\n",
    "    knowledge_df['similarity'] = knowledge_df['embedding'].apply(lambda x: cosine_similarity(x, embedding))\n",
    "    return knowledge_df.sort_values(by='similarity', ascending=False).head(k)\n",
    "\n",
    "def train_of_thought(texts,k,i=0):\n",
    "    if k>0:\n",
    "        longtxt = \"\".join(texts)\n",
    "        ctx = get_top_k_context(longtxt, i+1)\n",
    "        texts.append(\"\\nContext: \"+ctx['path'].iloc[i] + \" Data: \" + ctx['data'].iloc[i])\n",
    "        return train_of_thought(texts, k-1, i+1)\n",
    "    else:\n",
    "        return \"\".join(texts)\n",
    "    \n",
    "def ask_assistant(query, ctx, client:openai.OpenAI, thread):\n",
    "    #use context as system messages\n",
    "    ctx_messages = \"\\n\".join((\"CONTEXT:\\nPath: \"+ctx['path']+ \"\\nData: \" + ctx['data']+\"\\n\").tolist())\n",
    "    message = ctx_messages + \"\\nUSER: \" + query\n",
    "    print(message)\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, content=message,role='user')\n",
    "\n",
    "def handle_user_query(\n",
    "        query,\n",
    "        knowledge_df,\n",
    "        client:openai.OpenAI,\n",
    "        thread,\n",
    "        k=8,\n",
    "):\n",
    "    ctx = get_top_k_context(knowledge_df, query, k)\n",
    "    ask_assistant(query, ctx, client, thread)\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=os.getenv('OPENAI_ASSISTANT_ID')\n",
    "    )\n",
    "    while run.status != 'completed':\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "        time.sleep(1)\n",
    "    response = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "    return response\n",
    "\n",
    "def get_repo_name(repo_url):\n",
    "    splt1 = repo_url.split('/')\n",
    "    splt2 = splt1[-1].split('.')\n",
    "    if len(splt2) > 1:\n",
    "        return splt2[0]\n",
    "    else:\n",
    "        return splt1[-1]\n",
    "\n",
    "def clone_repo(repo_url):\n",
    "    os.system(\"git clone \" + repo_url + \" codebase_input/\" + get_repo_name(repo_url))\n",
    "\n",
    "def wipe_input_dir():\n",
    "    shutil.rmtree('codebase_input', ignore_errors=True)\n",
    "    os.mkdir('codebase_input')\n",
    "\n",
    "def update_codebase(repos, path_out='knowledge_df.csv', ext=['.py']):\n",
    "    #wipe_input_dir()\n",
    "    #for repo in repos:\n",
    "    #    clone_repo(repo)\n",
    "    generate_knowledge_from_dir('codebase_input', path_out, ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def check_file_extension(self, archive: str) -> bool:\n",
      "    logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "    extension = archive.split('.')[-1]\n",
      "    return extension in self.zip_files_formats\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def __init__(self):\n",
      "    self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "    self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "    self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "    self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "    logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_files_to_archive.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    pack_file = CompressFile()\n",
      "    output_zip_file_path = 'test/images_details.zip'\n",
      "    to_compress_dir_path = 'test'\n",
      "    pack_file.compress(output_zip_file=output_zip_file_path, dir_path=to_compress_dir_path)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    unpack_ = UnpackFile()\n",
      "    archive_path = config.settings['unpack_file']['compressed_file_path']\n",
      "    output = config.settings['unpack_file']['output_path'] + archive_path.replace('\\\\', '/').split('/')[-1].split('.')[0]\n",
      "    out = unpack_.extract_files(archive_path, output)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py\n",
      "Data: from shutil import unpack_archive\n",
      "import patoolib\n",
      "import glob\n",
      "import os\n",
      "import subprocess\n",
      "import string\n",
      "from logger.logger import logger\n",
      "from config import config\n",
      "\n",
      "class UnpackFile:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "        self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "        self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "        self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "        logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "    def extract_files(self, archive: str, output_path: str) -> list:\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - File does not exist: {archive}')\n",
      "            return []\n",
      "        if not self.check_file_extension(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - Not a archive: {archive}')\n",
      "            return []\n",
      "        archive = self.remove_special_characters_from_filename(archive)\n",
      "        logger.log_info(f'{__name__} - extract_files - New file name: {archive}')\n",
      "        logger.log_info(f'{__name__} - extract_files -Extracting files from: {archive}')\n",
      "        self.extract(archive, output_path)\n",
      "        return self.get_compressed_files(output_path)\n",
      "\n",
      "    def check_file_extension(self, archive: str) -> bool:\n",
      "        logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "        extension = archive.split('.')[-1]\n",
      "        return extension in self.zip_files_formats\n",
      "\n",
      "    def get_compressed_files(self, path_to_files: str) -> list:\n",
      "        counter, i, output = (0, 0, [])\n",
      "        while True:\n",
      "            if i >= self.max_nr_subfolders:\n",
      "                return output\n",
      "            lista = glob.glob(path_to_files + '/*.*' * i)\n",
      "            if not lista:\n",
      "                counter += 1\n",
      "            if counter >= 3:\n",
      "                logger.log_info(f'{__name__} - get_compressed_files - Files get: {output}')\n",
      "                return output\n",
      "            output += lista\n",
      "            i += 1\n",
      "\n",
      "    def extract(self, archive: str, output_path: str):\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract - File does not exist: {archive}')\n",
      "            return ''\n",
      "        if not os.path.isdir(output_path):\n",
      "            os.makedirs(output_path)\n",
      "        if self.algorithm == 1:\n",
      "            self.seven_zip_extract_files(archive=archive, output_path=output_path)\n",
      "        elif self.algorithm == 2:\n",
      "            self.patool_extract(archive=archive, output_path=output_path)\n",
      "\n",
      "    def seven_zip_extract_files(self, *args, **kwargs) -> dict:\n",
      "        logger.log_info(f\"{__name__} - seven_zip_extract_files - 7zip extract archive: {kwargs['archive']}\")\n",
      "        command = '7z e {0} -o{1}'.format(kwargs['archive'], kwargs['output_path'])\n",
      "        stdout = self.start_subprocess_popen(command=command)\n",
      "        return stdout\n",
      "\n",
      "    def patool_extract(self, *args, **kwargs) -> bool:\n",
      "        try:\n",
      "            patoolib.extract_archive(kwargs['archive'], outdir=kwargs['output_path'])\n",
      "        except Exception as e:\n",
      "            logger.log_error(f\"{__name__} - patool_extract - Cannot extract files from: {kwargs['archive']} [ERROR]: {e}\")\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def start_subprocess_popen(self, command: str) -> bytes:\n",
      "        stdout = ''\n",
      "        try:\n",
      "            args = command.split()\n",
      "            p = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "            stdout = p.communicate(timeout=self.unpack_max_time)[0]\n",
      "            logger.log_info(f'{__name__} - start_subprocess_popen - STDOUT: {stdout}')\n",
      "        except subprocess.TimeoutExpired as e:\n",
      "            logger.log_error(f'{__name__} - start_subprocess_popen - Expired time. {e}')\n",
      "        return stdout\n",
      "\n",
      "    def set_files_format(self, files_format: str) -> tuple:\n",
      "        logger.log_info(f'{__name__} - set_files_format - Setting the used COMPRESSED files formats')\n",
      "        formats = []\n",
      "        for file_format in files_format:\n",
      "            formats += [file_format.lower(), file_format.upper()]\n",
      "        logger.log_info(f'{__name__} - set_files_format - Successfuly set the used COMPRESSED files formats: {formats}')\n",
      "        return tuple(formats)\n",
      "\n",
      "    def remove_special_characters_from_filename(self, input_file_path: str='') -> str:\n",
      "        \"\"\"\n",
      "        Cleaning the file name from unknown characters\n",
      "        \"\"\"\n",
      "        if not os.path.isfile(input_file_path):\n",
      "            return input_file_path\n",
      "        path_to_file = '/'.join(input_file_path.replace('\\\\', '/').split('/')[:-1])\n",
      "        file_name = input_file_path.replace('\\\\', '/').split('/')[-1]\n",
      "        list_with_used_characters = list(string.digits + string.ascii_lowercase + string.ascii_uppercase + string.punctuation)\n",
      "        try:\n",
      "            unique_characters = list(set(file_name))\n",
      "        except Exception as e:\n",
      "            return input_file_path\n",
      "        for character in unique_characters:\n",
      "            if character in list_with_used_characters:\n",
      "                continue\n",
      "            file_name = file_name.replace(character, '_')\n",
      "        new_file_name = path_to_file + '/' + file_name\n",
      "        os.rename(input_file_path, new_file_name)\n",
      "        if not os.path.isfile(new_file_name):\n",
      "            return input_file_path\n",
      "        return new_file_name\n",
      "if __name__ == '__main__':\n",
      "    unpack_ = UnpackFile()\n",
      "    archive_path = config.settings['unpack_file']['compressed_file_path']\n",
      "    output = config.settings['unpack_file']['output_path'] + archive_path.replace('\\\\', '/').split('/')[-1].split('.')[0]\n",
      "    out = unpack_.extract_files(archive_path, output)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-file-to-image\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def check_file_extension(self, archive: str) -> bool:\n",
      "    logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "    extension = archive.split('.')[-1]\n",
      "    return extension in self.zip_files_formats\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\send_details.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    import glob\n",
      "    p = SendDetails()\n",
      "    files_path = '/home/solutions/data/docker_instances/solutions/fs/data/ecf-invoices-classifier-input/2023-11-06/sent_zip_files/'\n",
      "    files = glob.glob(files_path + '*.zip')\n",
      "    for file_name in files:\n",
      "        p.test_post_payload(file_path=file_name)\n",
      "        sleep(10)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef\n",
      "Data: class UnpackFile:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "        self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "        self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "        self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "        logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "    def extract_files(self, archive: str, output_path: str) -> list:\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - File does not exist: {archive}')\n",
      "            return []\n",
      "        if not self.check_file_extension(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - Not a archive: {archive}')\n",
      "            return []\n",
      "        archive = self.remove_special_characters_from_filename(archive)\n",
      "        logger.log_info(f'{__name__} - extract_files - New file name: {archive}')\n",
      "        logger.log_info(f'{__name__} - extract_files -Extracting files from: {archive}')\n",
      "        self.extract(archive, output_path)\n",
      "        return self.get_compressed_files(output_path)\n",
      "\n",
      "    def check_file_extension(self, archive: str) -> bool:\n",
      "        logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "        extension = archive.split('.')[-1]\n",
      "        return extension in self.zip_files_formats\n",
      "\n",
      "    def get_compressed_files(self, path_to_files: str) -> list:\n",
      "        counter, i, output = (0, 0, [])\n",
      "        while True:\n",
      "            if i >= self.max_nr_subfolders:\n",
      "                return output\n",
      "            lista = glob.glob(path_to_files + '/*.*' * i)\n",
      "            if not lista:\n",
      "                counter += 1\n",
      "            if counter >= 3:\n",
      "                logger.log_info(f'{__name__} - get_compressed_files - Files get: {output}')\n",
      "                return output\n",
      "            output += lista\n",
      "            i += 1\n",
      "\n",
      "    def extract(self, archive: str, output_path: str):\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract - File does not exist: {archive}')\n",
      "            return ''\n",
      "        if not os.path.isdir(output_path):\n",
      "            os.makedirs(output_path)\n",
      "        if self.algorithm == 1:\n",
      "            self.seven_zip_extract_files(archive=archive, output_path=output_path)\n",
      "        elif self.algorithm == 2:\n",
      "            self.patool_extract(archive=archive, output_path=output_path)\n",
      "\n",
      "    def seven_zip_extract_files(self, *args, **kwargs) -> dict:\n",
      "        logger.log_info(f\"{__name__} - seven_zip_extract_files - 7zip extract archive: {kwargs['archive']}\")\n",
      "        command = '7z e {0} -o{1}'.format(kwargs['archive'], kwargs['output_path'])\n",
      "        stdout = self.start_subprocess_popen(command=command)\n",
      "        return stdout\n",
      "\n",
      "    def patool_extract(self, *args, **kwargs) -> bool:\n",
      "        try:\n",
      "            patoolib.extract_archive(kwargs['archive'], outdir=kwargs['output_path'])\n",
      "        except Exception as e:\n",
      "            logger.log_error(f\"{__name__} - patool_extract - Cannot extract files from: {kwargs['archive']} [ERROR]: {e}\")\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def start_subprocess_popen(self, command: str) -> bytes:\n",
      "        stdout = ''\n",
      "        try:\n",
      "            args = command.split()\n",
      "            p = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "            stdout = p.communicate(timeout=self.unpack_max_time)[0]\n",
      "            logger.log_info(f'{__name__} - start_subprocess_popen - STDOUT: {stdout}')\n",
      "        except subprocess.TimeoutExpired as e:\n",
      "            logger.log_error(f'{__name__} - start_subprocess_popen - Expired time. {e}')\n",
      "        return stdout\n",
      "\n",
      "    def set_files_format(self, files_format: str) -> tuple:\n",
      "        logger.log_info(f'{__name__} - set_files_format - Setting the used COMPRESSED files formats')\n",
      "        formats = []\n",
      "        for file_format in files_format:\n",
      "            formats += [file_format.lower(), file_format.upper()]\n",
      "        logger.log_info(f'{__name__} - set_files_format - Successfuly set the used COMPRESSED files formats: {formats}')\n",
      "        return tuple(formats)\n",
      "\n",
      "    def remove_special_characters_from_filename(self, input_file_path: str='') -> str:\n",
      "        \"\"\"\n",
      "        Cleaning the file name from unknown characters\n",
      "        \"\"\"\n",
      "        if not os.path.isfile(input_file_path):\n",
      "            return input_file_path\n",
      "        path_to_file = '/'.join(input_file_path.replace('\\\\', '/').split('/')[:-1])\n",
      "        file_name = input_file_path.replace('\\\\', '/').split('/')[-1]\n",
      "        list_with_used_characters = list(string.digits + string.ascii_lowercase + string.ascii_uppercase + string.punctuation)\n",
      "        try:\n",
      "            unique_characters = list(set(file_name))\n",
      "        except Exception as e:\n",
      "            return input_file_path\n",
      "        for character in unique_characters:\n",
      "            if character in list_with_used_characters:\n",
      "                continue\n",
      "            file_name = file_name.replace(character, '_')\n",
      "        new_file_name = path_to_file + '/' + file_name\n",
      "        os.rename(input_file_path, new_file_name)\n",
      "        if not os.path.isfile(new_file_name):\n",
      "            return input_file_path\n",
      "        return new_file_name\n",
      "\n",
      "USER: Write a unit test for the check_file_extension, in the UnpackFile class, in the orchestrator module. Take into account the class it's a part of.\n"
     ]
    }
   ],
   "source": [
    "repos = [\n",
    "    \"https://github.com/mobutu/ecf-srdf-service-orchestrator\",\n",
    "    \"https://github.com/mobutu/ecf-srdf-service-file-to-image\",\n",
    "    \"https://github.com/Deathtanium/ecf-srdf-service-image-optimizer/\",\n",
    "    \"https://github.com/mobutu/ecf-srdf-service-iocr\",\n",
    "    \"https://github.com/mobutu/ecf-srdf-service-file-classifier\",\n",
    "    \"https://github.com/mobutu/ecf-srdf-service-details-extractor\"\n",
    "]\n",
    "update_codebase(repos)\n",
    "knowledge_df = load_knowledge('knowledge_df.csv')\n",
    "\n",
    "client = openai.OpenAI()\n",
    "assistant = client.beta.assistants.retrieve(os.getenv('OPENAI_ASSISTANT_ID'))\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "tosend = \"Write a unit test for the check_file_extension, in the UnpackFile class, in the orchestrator module. Take into account the class it's a part of.\"\n",
    "#tosend = input(\"Enter a message: \")\n",
    "\n",
    "response = handle_user_query(tosend, knowledge_df, client, thread, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unit tests have passed successfully. Both tests, `test_check_file_extension_valid` and `test_check_file_extension_invalid`, have confirmed that the `check_file_extension` method in the `UnpackFile` class correctly identifies valid and invalid file extensions, based on the mocked behavior that reflects the provided class context. If you need the actual test code to use in your project, please let me know, and I will provide it for you.\n",
      "It appears that the test `test_check_file_extension_valid` has failed. This could be due to the mocked `UnpackFile` class not matching the behavior of the actual class from the provided context. Let's try to closely match the actual class behavior in our mocked version and run the test again. \n",
      "\n",
      "I will modify the test to include the correct expected behavior and run it again.\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def check_file_extension(self, archive: str) -> bool:\n",
      "    logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "    extension = archive.split('.')[-1]\n",
      "    return extension in self.zip_files_formats\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def __init__(self):\n",
      "    self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "    self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "    self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "    self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "    logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_files_to_archive.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    pack_file = CompressFile()\n",
      "    output_zip_file_path = 'test/images_details.zip'\n",
      "    to_compress_dir_path = 'test'\n",
      "    pack_file.compress(output_zip_file=output_zip_file_path, dir_path=to_compress_dir_path)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    unpack_ = UnpackFile()\n",
      "    archive_path = config.settings['unpack_file']['compressed_file_path']\n",
      "    output = config.settings['unpack_file']['output_path'] + archive_path.replace('\\\\', '/').split('/')[-1].split('.')[0]\n",
      "    out = unpack_.extract_files(archive_path, output)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py\n",
      "Data: from shutil import unpack_archive\n",
      "import patoolib\n",
      "import glob\n",
      "import os\n",
      "import subprocess\n",
      "import string\n",
      "from logger.logger import logger\n",
      "from config import config\n",
      "\n",
      "class UnpackFile:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "        self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "        self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "        self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "        logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "    def extract_files(self, archive: str, output_path: str) -> list:\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - File does not exist: {archive}')\n",
      "            return []\n",
      "        if not self.check_file_extension(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - Not a archive: {archive}')\n",
      "            return []\n",
      "        archive = self.remove_special_characters_from_filename(archive)\n",
      "        logger.log_info(f'{__name__} - extract_files - New file name: {archive}')\n",
      "        logger.log_info(f'{__name__} - extract_files -Extracting files from: {archive}')\n",
      "        self.extract(archive, output_path)\n",
      "        return self.get_compressed_files(output_path)\n",
      "\n",
      "    def check_file_extension(self, archive: str) -> bool:\n",
      "        logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "        extension = archive.split('.')[-1]\n",
      "        return extension in self.zip_files_formats\n",
      "\n",
      "    def get_compressed_files(self, path_to_files: str) -> list:\n",
      "        counter, i, output = (0, 0, [])\n",
      "        while True:\n",
      "            if i >= self.max_nr_subfolders:\n",
      "                return output\n",
      "            lista = glob.glob(path_to_files + '/*.*' * i)\n",
      "            if not lista:\n",
      "                counter += 1\n",
      "            if counter >= 3:\n",
      "                logger.log_info(f'{__name__} - get_compressed_files - Files get: {output}')\n",
      "                return output\n",
      "            output += lista\n",
      "            i += 1\n",
      "\n",
      "    def extract(self, archive: str, output_path: str):\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract - File does not exist: {archive}')\n",
      "            return ''\n",
      "        if not os.path.isdir(output_path):\n",
      "            os.makedirs(output_path)\n",
      "        if self.algorithm == 1:\n",
      "            self.seven_zip_extract_files(archive=archive, output_path=output_path)\n",
      "        elif self.algorithm == 2:\n",
      "            self.patool_extract(archive=archive, output_path=output_path)\n",
      "\n",
      "    def seven_zip_extract_files(self, *args, **kwargs) -> dict:\n",
      "        logger.log_info(f\"{__name__} - seven_zip_extract_files - 7zip extract archive: {kwargs['archive']}\")\n",
      "        command = '7z e {0} -o{1}'.format(kwargs['archive'], kwargs['output_path'])\n",
      "        stdout = self.start_subprocess_popen(command=command)\n",
      "        return stdout\n",
      "\n",
      "    def patool_extract(self, *args, **kwargs) -> bool:\n",
      "        try:\n",
      "            patoolib.extract_archive(kwargs['archive'], outdir=kwargs['output_path'])\n",
      "        except Exception as e:\n",
      "            logger.log_error(f\"{__name__} - patool_extract - Cannot extract files from: {kwargs['archive']} [ERROR]: {e}\")\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def start_subprocess_popen(self, command: str) -> bytes:\n",
      "        stdout = ''\n",
      "        try:\n",
      "            args = command.split()\n",
      "            p = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "            stdout = p.communicate(timeout=self.unpack_max_time)[0]\n",
      "            logger.log_info(f'{__name__} - start_subprocess_popen - STDOUT: {stdout}')\n",
      "        except subprocess.TimeoutExpired as e:\n",
      "            logger.log_error(f'{__name__} - start_subprocess_popen - Expired time. {e}')\n",
      "        return stdout\n",
      "\n",
      "    def set_files_format(self, files_format: str) -> tuple:\n",
      "        logger.log_info(f'{__name__} - set_files_format - Setting the used COMPRESSED files formats')\n",
      "        formats = []\n",
      "        for file_format in files_format:\n",
      "            formats += [file_format.lower(), file_format.upper()]\n",
      "        logger.log_info(f'{__name__} - set_files_format - Successfuly set the used COMPRESSED files formats: {formats}')\n",
      "        return tuple(formats)\n",
      "\n",
      "    def remove_special_characters_from_filename(self, input_file_path: str='') -> str:\n",
      "        \"\"\"\n",
      "        Cleaning the file name from unknown characters\n",
      "        \"\"\"\n",
      "        if not os.path.isfile(input_file_path):\n",
      "            return input_file_path\n",
      "        path_to_file = '/'.join(input_file_path.replace('\\\\', '/').split('/')[:-1])\n",
      "        file_name = input_file_path.replace('\\\\', '/').split('/')[-1]\n",
      "        list_with_used_characters = list(string.digits + string.ascii_lowercase + string.ascii_uppercase + string.punctuation)\n",
      "        try:\n",
      "            unique_characters = list(set(file_name))\n",
      "        except Exception as e:\n",
      "            return input_file_path\n",
      "        for character in unique_characters:\n",
      "            if character in list_with_used_characters:\n",
      "                continue\n",
      "            file_name = file_name.replace(character, '_')\n",
      "        new_file_name = path_to_file + '/' + file_name\n",
      "        os.rename(input_file_path, new_file_name)\n",
      "        if not os.path.isfile(new_file_name):\n",
      "            return input_file_path\n",
      "        return new_file_name\n",
      "if __name__ == '__main__':\n",
      "    unpack_ = UnpackFile()\n",
      "    archive_path = config.settings['unpack_file']['compressed_file_path']\n",
      "    output = config.settings['unpack_file']['output_path'] + archive_path.replace('\\\\', '/').split('/')[-1].split('.')[0]\n",
      "    out = unpack_.extract_files(archive_path, output)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-file-to-image\\src\\convertor_archive_to_files.py>ClassDef>FunctionDef\n",
      "Data: def check_file_extension(self, archive: str) -> bool:\n",
      "    logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "    extension = archive.split('.')[-1]\n",
      "    return extension in self.zip_files_formats\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\send_details.py>If\n",
      "Data: if __name__ == '__main__':\n",
      "    import glob\n",
      "    p = SendDetails()\n",
      "    files_path = '/home/solutions/data/docker_instances/solutions/fs/data/ecf-invoices-classifier-input/2023-11-06/sent_zip_files/'\n",
      "    files = glob.glob(files_path + '*.zip')\n",
      "    for file_name in files:\n",
      "        p.test_post_payload(file_path=file_name)\n",
      "        sleep(10)\n",
      "\n",
      "CONTEXT:\n",
      "Path: codebase_input\\ecf-srdf-service-orchestrator\\src\\convertor_archive_to_files.py>ClassDef\n",
      "Data: class UnpackFile:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.zip_files_formats = self.set_files_format(config.settings['unpack_file']['supported_formats'])\n",
      "        self.max_nr_subfolders = config.settings['unpack_file']['max_nr_subfolders']\n",
      "        self.unpack_max_time = config.settings['unpack_file']['unpack_max_time']\n",
      "        self.algorithm = config.settings['unpack_file']['algorithm']\n",
      "        logger.log_info(f'{__name__} - __init__ - Successfully set the UnpackFile')\n",
      "\n",
      "    def extract_files(self, archive: str, output_path: str) -> list:\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - File does not exist: {archive}')\n",
      "            return []\n",
      "        if not self.check_file_extension(archive):\n",
      "            logger.log_warning(f'{__name__} - extract_files - Not a archive: {archive}')\n",
      "            return []\n",
      "        archive = self.remove_special_characters_from_filename(archive)\n",
      "        logger.log_info(f'{__name__} - extract_files - New file name: {archive}')\n",
      "        logger.log_info(f'{__name__} - extract_files -Extracting files from: {archive}')\n",
      "        self.extract(archive, output_path)\n",
      "        return self.get_compressed_files(output_path)\n",
      "\n",
      "    def check_file_extension(self, archive: str) -> bool:\n",
      "        logger.log_info(f'{__name__} - check_file_extension - Processing file: {archive}')\n",
      "        extension = archive.split('.')[-1]\n",
      "        return extension in self.zip_files_formats\n",
      "\n",
      "    def get_compressed_files(self, path_to_files: str) -> list:\n",
      "        counter, i, output = (0, 0, [])\n",
      "        while True:\n",
      "            if i >= self.max_nr_subfolders:\n",
      "                return output\n",
      "            lista = glob.glob(path_to_files + '/*.*' * i)\n",
      "            if not lista:\n",
      "                counter += 1\n",
      "            if counter >= 3:\n",
      "                logger.log_info(f'{__name__} - get_compressed_files - Files get: {output}')\n",
      "                return output\n",
      "            output += lista\n",
      "            i += 1\n",
      "\n",
      "    def extract(self, archive: str, output_path: str):\n",
      "        if not os.path.isfile(archive):\n",
      "            logger.log_warning(f'{__name__} - extract - File does not exist: {archive}')\n",
      "            return ''\n",
      "        if not os.path.isdir(output_path):\n",
      "            os.makedirs(output_path)\n",
      "        if self.algorithm == 1:\n",
      "            self.seven_zip_extract_files(archive=archive, output_path=output_path)\n",
      "        elif self.algorithm == 2:\n",
      "            self.patool_extract(archive=archive, output_path=output_path)\n",
      "\n",
      "    def seven_zip_extract_files(self, *args, **kwargs) -> dict:\n",
      "        logger.log_info(f\"{__name__} - seven_zip_extract_files - 7zip extract archive: {kwargs['archive']}\")\n",
      "        command = '7z e {0} -o{1}'.format(kwargs['archive'], kwargs['output_path'])\n",
      "        stdout = self.start_subprocess_popen(command=command)\n",
      "        return stdout\n",
      "\n",
      "    def patool_extract(self, *args, **kwargs) -> bool:\n",
      "        try:\n",
      "            patoolib.extract_archive(kwargs['archive'], outdir=kwargs['output_path'])\n",
      "        except Exception as e:\n",
      "            logger.log_error(f\"{__name__} - patool_extract - Cannot extract files from: {kwargs['archive']} [ERROR]: {e}\")\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def start_subprocess_popen(self, command: str) -> bytes:\n",
      "        stdout = ''\n",
      "        try:\n",
      "            args = command.split()\n",
      "            p = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
      "            stdout = p.communicate(timeout=self.unpack_max_time)[0]\n",
      "            logger.log_info(f'{__name__} - start_subprocess_popen - STDOUT: {stdout}')\n",
      "        except subprocess.TimeoutExpired as e:\n",
      "            logger.log_error(f'{__name__} - start_subprocess_popen - Expired time. {e}')\n",
      "        return stdout\n",
      "\n",
      "    def set_files_format(self, files_format: str) -> tuple:\n",
      "        logger.log_info(f'{__name__} - set_files_format - Setting the used COMPRESSED files formats')\n",
      "        formats = []\n",
      "        for file_format in files_format:\n",
      "            formats += [file_format.lower(), file_format.upper()]\n",
      "        logger.log_info(f'{__name__} - set_files_format - Successfuly set the used COMPRESSED files formats: {formats}')\n",
      "        return tuple(formats)\n",
      "\n",
      "    def remove_special_characters_from_filename(self, input_file_path: str='') -> str:\n",
      "        \"\"\"\n",
      "        Cleaning the file name from unknown characters\n",
      "        \"\"\"\n",
      "        if not os.path.isfile(input_file_path):\n",
      "            return input_file_path\n",
      "        path_to_file = '/'.join(input_file_path.replace('\\\\', '/').split('/')[:-1])\n",
      "        file_name = input_file_path.replace('\\\\', '/').split('/')[-1]\n",
      "        list_with_used_characters = list(string.digits + string.ascii_lowercase + string.ascii_uppercase + string.punctuation)\n",
      "        try:\n",
      "            unique_characters = list(set(file_name))\n",
      "        except Exception as e:\n",
      "            return input_file_path\n",
      "        for character in unique_characters:\n",
      "            if character in list_with_used_characters:\n",
      "                continue\n",
      "            file_name = file_name.replace(character, '_')\n",
      "        new_file_name = path_to_file + '/' + file_name\n",
      "        os.rename(input_file_path, new_file_name)\n",
      "        if not os.path.isfile(new_file_name):\n",
      "            return input_file_path\n",
      "        return new_file_name\n",
      "\n",
      "USER: Write a unit test for the check_file_extension, in the UnpackFile class, in the orchestrator module. Take into account the class it's a part of.\n"
     ]
    }
   ],
   "source": [
    "for message in response.data:\n",
    "    print(message.content[0].text.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
